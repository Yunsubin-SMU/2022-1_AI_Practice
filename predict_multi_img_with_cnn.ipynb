{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee5d03b",
   "metadata": {},
   "source": [
    "# 멀티라벨 분류\n",
    "일단 유제품과 면류 2가지 이미지를 분류한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af2d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b241b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.2-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from matplotlib) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from matplotlib) (9.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\soobi\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.33.3 kiwisolver-1.4.2 matplotlib-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76158ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dairy  파일 길이 :  2027\n",
      "dairy  :  ./dataset/train_data/dairy\\10246_00_m_14.jpg\n",
      "dairy  :  ./dataset/train_data/dairy\\20243_30_s_9.jpg\n",
      "dairy  :  ./dataset/train_data/dairy\\45132_00_s_22.jpg\n",
      "noodle  파일 길이 :  1335\n",
      "noodle  :  ./dataset/train_data/noodle\\10101_00_m_10.jpg\n",
      "noodle  :  ./dataset/train_data/noodle\\60110_60_m_21.jpg\n",
      "ok 3362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soobi\\miniconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"./dataset/train_data\"\n",
    "categories = [\"dairy\", \"noodle\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#1 0 이면 dairy\n",
    "#0 1 이면 noodle 이런식\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a73b277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2521, 64, 64, 3)\n",
      "2521\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./multi_image_data.npy', allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d251079",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"dairy\", \"noodle\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc37efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_dir = './model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c60e781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4194560   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,214,466\n",
      "Trainable params: 4,214,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59f660cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7790 - accuracy: 0.5871\n",
      "Epoch 1: val_loss improved from inf to 0.61033, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 6s 76ms/step - loss: 0.7790 - accuracy: 0.5871 - val_loss: 0.6103 - val_accuracy: 0.6718\n",
      "Epoch 2/50\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.5847 - accuracy: 0.6911\n",
      "Epoch 2: val_loss improved from 0.61033 to 0.56022, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 0.5847 - accuracy: 0.6906 - val_loss: 0.5602 - val_accuracy: 0.7004\n",
      "Epoch 3/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.7196\n",
      "Epoch 3: val_loss improved from 0.56022 to 0.53548, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 0.5397 - accuracy: 0.7196 - val_loss: 0.5355 - val_accuracy: 0.7301\n",
      "Epoch 4/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.7410\n",
      "Epoch 4: val_loss improved from 0.53548 to 0.50418, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 7s 90ms/step - loss: 0.5143 - accuracy: 0.7410 - val_loss: 0.5042 - val_accuracy: 0.7432\n",
      "Epoch 5/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.7533\n",
      "Epoch 5: val_loss improved from 0.50418 to 0.48334, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 7s 93ms/step - loss: 0.5009 - accuracy: 0.7533 - val_loss: 0.4833 - val_accuracy: 0.7705\n",
      "Epoch 6/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.7640\n",
      "Epoch 6: val_loss improved from 0.48334 to 0.47687, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 96ms/step - loss: 0.4826 - accuracy: 0.7640 - val_loss: 0.4769 - val_accuracy: 0.7467\n",
      "Epoch 7/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.7739\n",
      "Epoch 7: val_loss improved from 0.47687 to 0.45140, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 98ms/step - loss: 0.4575 - accuracy: 0.7739 - val_loss: 0.4514 - val_accuracy: 0.7824\n",
      "Epoch 8/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4411 - accuracy: 0.7846\n",
      "Epoch 8: val_loss improved from 0.45140 to 0.42523, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 96ms/step - loss: 0.4411 - accuracy: 0.7846 - val_loss: 0.4252 - val_accuracy: 0.8050\n",
      "Epoch 9/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8048\n",
      "Epoch 9: val_loss improved from 0.42523 to 0.40497, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 98ms/step - loss: 0.4118 - accuracy: 0.8048 - val_loss: 0.4050 - val_accuracy: 0.8098\n",
      "Epoch 10/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3918 - accuracy: 0.8124\n",
      "Epoch 10: val_loss improved from 0.40497 to 0.40091, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 96ms/step - loss: 0.3918 - accuracy: 0.8124 - val_loss: 0.4009 - val_accuracy: 0.8205\n",
      "Epoch 11/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8310\n",
      "Epoch 11: val_loss improved from 0.40091 to 0.40090, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 100ms/step - loss: 0.3676 - accuracy: 0.8310 - val_loss: 0.4009 - val_accuracy: 0.8002\n",
      "Epoch 12/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.8401\n",
      "Epoch 12: val_loss improved from 0.40090 to 0.39128, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 99ms/step - loss: 0.3503 - accuracy: 0.8401 - val_loss: 0.3913 - val_accuracy: 0.8121\n",
      "Epoch 13/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.8497\n",
      "Epoch 13: val_loss did not improve from 0.39128\n",
      "79/79 [==============================] - 7s 90ms/step - loss: 0.3274 - accuracy: 0.8497 - val_loss: 0.3977 - val_accuracy: 0.8300\n",
      "Epoch 14/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8631\n",
      "Epoch 14: val_loss improved from 0.39128 to 0.36833, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 98ms/step - loss: 0.3104 - accuracy: 0.8631 - val_loss: 0.3683 - val_accuracy: 0.8276\n",
      "Epoch 15/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.8862\n",
      "Epoch 15: val_loss improved from 0.36833 to 0.34371, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 8s 101ms/step - loss: 0.2751 - accuracy: 0.8862 - val_loss: 0.3437 - val_accuracy: 0.8478\n",
      "Epoch 16/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.8953\n",
      "Epoch 16: val_loss improved from 0.34371 to 0.33960, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 10s 123ms/step - loss: 0.2444 - accuracy: 0.8953 - val_loss: 0.3396 - val_accuracy: 0.8621\n",
      "Epoch 17/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.8969\n",
      "Epoch 17: val_loss did not improve from 0.33960\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.2383 - accuracy: 0.8969 - val_loss: 0.3411 - val_accuracy: 0.8597\n",
      "Epoch 18/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9044\n",
      "Epoch 18: val_loss did not improve from 0.33960\n",
      "79/79 [==============================] - 10s 123ms/step - loss: 0.2269 - accuracy: 0.9044 - val_loss: 0.3595 - val_accuracy: 0.8561\n",
      "Epoch 19/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9159\n",
      "Epoch 19: val_loss improved from 0.33960 to 0.33637, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 11s 139ms/step - loss: 0.2069 - accuracy: 0.9159 - val_loss: 0.3364 - val_accuracy: 0.8621\n",
      "Epoch 20/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.9175\n",
      "Epoch 20: val_loss did not improve from 0.33637\n",
      "79/79 [==============================] - 11s 137ms/step - loss: 0.1895 - accuracy: 0.9175 - val_loss: 0.3603 - val_accuracy: 0.8549\n",
      "Epoch 21/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9298\n",
      "Epoch 21: val_loss improved from 0.33637 to 0.32305, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 0.1747 - accuracy: 0.9298 - val_loss: 0.3231 - val_accuracy: 0.8906\n",
      "Epoch 22/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9389\n",
      "Epoch 22: val_loss did not improve from 0.32305\n",
      "79/79 [==============================] - 12s 152ms/step - loss: 0.1621 - accuracy: 0.9389 - val_loss: 0.3531 - val_accuracy: 0.8680\n",
      "Epoch 23/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9468\n",
      "Epoch 23: val_loss improved from 0.32305 to 0.32246, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 10s 126ms/step - loss: 0.1412 - accuracy: 0.9468 - val_loss: 0.3225 - val_accuracy: 0.8751\n",
      "Epoch 24/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9512\n",
      "Epoch 24: val_loss improved from 0.32246 to 0.29503, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "79/79 [==============================] - 9s 112ms/step - loss: 0.1247 - accuracy: 0.9512 - val_loss: 0.2950 - val_accuracy: 0.8859\n",
      "Epoch 25/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9580\n",
      "Epoch 25: val_loss did not improve from 0.29503\n",
      "79/79 [==============================] - 8s 100ms/step - loss: 0.1120 - accuracy: 0.9580 - val_loss: 0.3337 - val_accuracy: 0.8787\n",
      "Epoch 26/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9607\n",
      "Epoch 26: val_loss did not improve from 0.29503\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 0.1065 - accuracy: 0.9607 - val_loss: 0.3753 - val_accuracy: 0.8775\n",
      "Epoch 27/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9552\n",
      "Epoch 27: val_loss did not improve from 0.29503\n",
      "79/79 [==============================] - 7s 90ms/step - loss: 0.1159 - accuracy: 0.9552 - val_loss: 0.3877 - val_accuracy: 0.8704\n",
      "Epoch 28/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9663\n",
      "Epoch 28: val_loss did not improve from 0.29503\n",
      "79/79 [==============================] - 7s 93ms/step - loss: 0.0923 - accuracy: 0.9663 - val_loss: 0.3245 - val_accuracy: 0.8918\n",
      "Epoch 29/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9695\n",
      "Epoch 29: val_loss did not improve from 0.29503\n",
      "79/79 [==============================] - 8s 97ms/step - loss: 0.0871 - accuracy: 0.9695 - val_loss: 0.3251 - val_accuracy: 0.8751\n",
      "Epoch 30/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9702\n",
      "Epoch 30: val_loss did not improve from 0.29503\n",
      "79/79 [==============================] - 8s 103ms/step - loss: 0.0801 - accuracy: 0.9702 - val_loss: 0.3556 - val_accuracy: 0.8740\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "163137e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 16ms/step - loss: 0.3556 - accuracy: 0.8740\n",
      "정확도 : 0.8740\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "312f5f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757697a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E822C5FA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 10249_00_s_22.jpg이미지는 유제품로 추정됩니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 15132_00_s_24.jpg이미지는 유제품로 추정됩니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 15195_00_s_14.jpg이미지는 유제품로 추정됩니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 20113_0_s_3.jpg이미지는 유제품로 추정됩니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 20114_30_s_1.jpg이미지는 면류으로 추정됩니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 25053_30_s_23.jpg이미지는 면류으로 추정됩니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 25292_00_s_22.jpg이미지는 면류으로 추정됩니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 25766_30_s_11.jpg이미지는 유제품로 추정됩니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 40061_0_s_15.jpg이미지는 면류으로 추정됩니다.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "caltech_dir = \"./dataset/test_data\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(caltech_dir+\"/*.*\")\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "model = load_model('./model/multi_img_classification.model')\n",
    "\n",
    "prediction = model.predict(X)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"유제품\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"면류\"\n",
    "    #elif pre_ans == 2: pre_ans_str = \"나비\"\n",
    "    #else: pre_ans_str = \"게\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    #if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    #if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de05b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
